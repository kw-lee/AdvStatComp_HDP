
\section{Hierarchical-Block Approximations}\label{sec:hmvn}

In this section, we suggest methods to solve the $n$-dimensional MVN problem with the hierarchical covariance matrix using the $d$-dimensional conditioning method with that of the Monte Carlo-based method for solving the $m$-dimensional MVN problems presented by the diagonal blocks.
% 5. HCMVN - 경원 Hchol, CMVN, RCMVN, blocking

\subsection{Hierarchical Cholesky Decomposition}

\citet{hackbusch2015hierarchical} proposed hiarchical matrix and its cholesky decomposition method. We have applied low rank approximation to each block of its decomposition to make implementation efficiently and save storage while accuracy is preserved.
$A=LU$have the structure
$$\begin{pmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{pmatrix}=\begin{pmatrix}L_{11}&O\\L_{21}&L_{22}\end{pmatrix}\begin{pmatrix}L_{11}^T&L_{12}^T\\O&L_{22}^T\end{pmatrix}$$
with lower triangular matrix $L_{11},L_{22}$.
It leads to four tasks:
\begin{itemize}
	\item[(a)] compute $L_{11}$ via Cholesky decomposition of $A_{11}$
	\item[(b)] compute $L_{12}$ from $L_{21}L_{11}^T = A_{21}$
	\item[(c)] low rank approximation of $L_{12}=UV^T$
	\item[(d)] compute $L_{22}$ via Cholesky decomposition of $A_{22}-L_{21}L_{21}^T$
\end{itemize}
(a) and (d) is solved with hierachical cholesky decomposition it self, and (b) is easy since it has triangular form. (c) is implemented with low-rank approximation of SVD, i.e. $A=UDV^T=\sum_{i=1}^n d_i u_iv_i^T\approx\sum_{i=1}^k d_i u_iv_i^T$.
Hierachical cholesky decomposition of $n\times n$ matrix into $m\times m$ blocks is implemented like below.
\begin{algorithm}[H]
	\caption{Hierachical cholesky decomposition}
	\begin{algorithmic}[1]
		\Procedure{\texttt{hchol}}{$A$, n,m,rank}
		\For{$i=1:log_2(\frac{n}{m})$}
		\State $nb = n/2^i$
		\State x = 0, y = nb
		\For{$j=1:2^{i-1}$}
		\State $\mathbf{U,D,V} = lowrankSVD(A[xbegin+1:xbegin+nb,ybegin+1:ybegin+nb], rank)$
		\State $\mathbf{A}[x + 1:x + nb, y + 1:y + rank] = \mathbf{UD}$
		\State $\mathbf{A}[x + 1:x + nb, y + rank+1:y + nb] = \mathbf{O}$
		\State $\mathbf{A}[y + 1:y + nb, x + 1:x + rank] = \mathbf{VD}$
		\State $\mathbf{A}[y + 1:y + nb, x + rank+1:x + nb] = \mathbf{O}$
		\State $x += 2nb, y += 2nb$
		\EndFor
		\EndFor
		\EndProcedure
		
	\end{algorithmic}\label{alg:hchol}
\end{algorithm}


\subsection{The Hierarchical-Block Conditioning Method}

Let $\phi_m(\mathbf{x}; \boldsymbol{\Sigma})$ be a pdf of the $m$-dimensional normal distribution $N(\mathbf{0}, \boldsymbol{\Sigma})$ and $(\mathbf{B}, \mathbf{U}\mathbf{V}^T)$ be the hierarchical Cholesky decompostion of the covariance matrix $\boldsymbol{\Sigma}$. Then, we can express \eqref{eqn:normalprob} as 
\begin{equation}\label{eqn:hmvn}
    \Phi_n(\mathbf{a}, \mathbf{b}; \mathbf{0}, \boldsymbol{\Sigma}) 
    = \int_{\mathbf{a}_1'}^{\mathbf{b}_1'} \phi_m(\mathbf{x}_1; \mathbf{B}_1\mathbf{B}_1^T) 
    \cdots 
    \int_{\mathbf{a}_r'}^{\mathbf{b}_r'} \phi_r(\mathbf{x}_r; \mathbf{B}_r\mathbf{B}_r^T) d\mathbf{x}_r \cdots d\mathbf{x}_1.
\end{equation}
Where $\mathbf{a}',~\mathbf{b}'$, $i=1,\cdots,r$, are the corresponding segments of the updated $\mathbf{a}$ and $\mathbf{b}$. Specifficaly, we can compute $n$-dimensional MVN problem using hierarchical structure as algorithm \ref{alg:hmvn}.

\begin{algorithm}[H]
    \caption{Hierarchical-block conditioning algorithm}
    \begin{algorithmic}[1]
    
    \Procedure{\texttt{HMVN}}{$a,~b,~\boldsymbol{\Sigma},~d$}
        \State $\mathbf{x} \leftarrow \mathbf{0}$ and $P \leftarrow 1$
        \State $[\mathbf{B}, \mathbf{UV}] \leftarrow$ \texttt{choldecomp\_hmatrix}$(\boldsymbol{\Sigma})$
        \For{$i = 1:r$}
            \State $j \leftarrow (i-1)m$
            \If{$i>1$}
                \State $o_r \leftarrow$ row offset of $\mathbf{U}_{i-1}\mathbf{V}_{i-1}^T$
                \State $o_c \leftarrow$ column offset of $\mathbf{U}_{i-1}\mathbf{V}_{i-1}^T$
                \State $l \leftarrow \dim(\mathbf{U}_{i-1}\mathbf{V}_{i-1}^T)$
                \State $\mathbf{g} \leftarrow \mathbf{U}_{i-1}\mathbf{V}_{i-1}^T\mathbf{x}[o_c+1:o_c+l]$
                \State $\mathbf{a}[o_r+1:o_r+l] = \mathbf{a}[o_r+1:o_r+l] - \mathbf{g}$
                \State $\mathbf{b}[o_r+1:o_r+l] = \mathbf{a}[o_r+1:o_r+l] - \mathbf{g}$
            \EndIf
            \State $\mathbf{a}_i \leftarrow \mathbf{a}[j+1:j+m]$
            \State $\mathbf{b}_i \leftarrow \mathbf{b}[j+1:j+m]$
            \State $P = P*\Phi_m(\mathbf{a}_i, \mathbf{b}_i; \mathbf{0}, \mathbf{B}_i\mathbf{B}_i^T)$
            \State $\mathbf{x}[j+1:j+m] \leftarrow \mathbf{B}_{i}^{-1} E(\mathbf{X}_i)$
        \EndFor
    \EndProcedure

    \end{algorithmic}\label{alg:hmvn}
\end{algorithm}

Note the probabilities $\Phi_m(\mathbf{a}_i, \mathbf{b}_i; \mathbf{0}, \mathbf{B}_i\mathbf{B}_i^T)$ can be computed using Quasi-Monte Carlo method (\texttt{HMVN}, Method 1 in \citet{cao2019hierarchical}),  $d$-dimensional conditioning algorithm (\texttt{HCMVN}, Method 2 in \citet{cao2019hierarchical}) or with $d$-dimensional conditioning algorithm with univariate reordering (\texttt{HRCMVN}, Method 3 in \citet{cao2019hierarchical}). These methods are more effective and easily parallelizable than the classical methods.

\subsection{Computational Complexity}

For a clearer comparison of the complexities, we decompose the complexity of Algorithm \ref{alg:hmvn} into three parts and list the complexity for each part in Table \ref{tbl:cc_hmvn}, where $M(\cdot)$ denotes the complexity of the QMC simulation in the given dimension. 
%Table \ref{tbl:cc_hmvn} shows that the time efﬁciency of the $d$-dimensional conditioning algorithm mainly comes from lowering the dimension in which the QMC simulation is performed. 
\renewcommand{\arraystretch}{1.5}
\begin{table}[ht]
    \begin{center}
        \begin{tabular}{l l l l}
                            & MVN prob                     & Trunc exp          & Upd limits            \\
            \hline
            \texttt{HMVN}   & $\frac{n}{m} M(m)$           & $2nM(m) + O(nm^2)$ & $O(mn + kn log(n/m))$ \\
            \texttt{HCMVN}  & $\frac{n}{d} M(d) + O(m^2n)$ & $2nM(d) + O(nd^2)$ & $O(mn + kn log(n/m))$ \\
            \texttt{HRCMVN} & $\frac{n}{d} M(d) + O(m^2n)$ & $2nM(d) + O(nd^2)$ & $O(mn + kn log(n/m))$ \\
            \hline
        \end{tabular}
        \caption{Complexity decomposition of the \texttt{HMVN}, \texttt{HCMVN}, and \texttt{HRCMVN}}\label{tbl:cc_hmvn}
    \end{center}
\end{table}
\renewcommand{\arraystretch}{1}


The three parts of the complexity are the calculation of the MVN probability (MVN prob), the calculation of the truncated expectations (Trunc exp), and the update of the integration limits with truncated expectations (Upd limits). The latter two share the same asymptotic order in all three complexity terms. The updating cost is independent of the method. The complexity of the univariate reordering is $O(m^2 n)$, the same as the complexity of computing the MVN probabilities in \texttt{HCMVN}, resulting in an identical major complexity component for \texttt{HCMVN} and \texttt{HRCMVN}. Since \texttt{HCMVN} and \texttt{HRCMVN} perform the QMC  simulation in $d$-dimensions, these two methods are not greatly affected by the choice of $m$.