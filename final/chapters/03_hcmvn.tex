
\section{Hierarchical-Block Approximations}\label{sec:hmvn}

% 5. HCMVN - 경원 Hchol, CMVN, RCMVN, blocking

\subsection{The Hierarchical-Block Conditioning Method}

In this section, we suggest methods to solve the $n$-dimensional MVN problem with the hierarchical covariance matrix using the $d$-dimensional conditioning method with that of the Monte Carlo-based method for solving the $m$-dimensional MVN problems presented by the diagonal blocks.

Let $\phi_m(\mathbf{x}; \boldsymbol{\Sigma})$ be a pdf of the $m$-dimensional normal distribution $N(\mathbf{0}, \boldsymbol{\Sigma})$ and $(\mathbf{B}, \mathbf{U}\mathbf{V}^T)$ be the hierarchical Cholesky decompostion of the covariance matrix $\Sigma$. Then, we can express \eqref{eqn:normalprob} as 
\begin{equation}\label{eqn:hmvn}
    \Phi_n(\mathbf{a}, \mathbf{b}; \mathbf{0}, \boldsymbol{\Sigma}) 
    = \int_{\mathbf{a}_1'}^{\mathbf{b}_1'} \phi_m(\mathbf{x}_1; \mathbf{B}_1\mathbf{B}_1^T) 
    \cdots 
    \int_{\mathbf{a}_r'}^{\mathbf{b}_r'} \phi_r(\mathbf{x}_r; \mathbf{B}_r\mathbf{B}_r^T) d\mathbf{x}_r \cdots d\mathbf{x}_1.
\end{equation}
Where $\mathbf{a}',~\mathbf{b}'$, $i=1,\cdots,r$, are the corresponding segments of the updated $\mathbf{a}$ and $\mathbf{b}$. Specifficaly, we can compute $n$-dimensional MVN problem using hierarchical structure as algorithm \ref{alg:hmvn}.

\begin{algorithm}[H]
    \caption{Hierarchical-block conditioning algorithm}
    \begin{algorithmic}[1]
    
    \Procedure{\texttt{HMVN}}{$a,~b,~\Sigma,~d$}
        \State $\mathbf{x} \leftarrow \mathbf{0}$ and $P \leftarrow 1$
        \State $[\mathbf{B}, \mathbf{UV}] \leftarrow$ \texttt{choldecomp\_hmatrix}$(\Sigma)$
        \For{$i = 1:r$}
            \State $j \leftarrow (i-1)m$
            \If{$i>1$}
                \State $o_r \leftarrow$ row offset of $\mathbf{U}_{i-1}\mathbf{V}_{i-1}^T$
                \State $o_c \leftarrow$ column offset of $\mathbf{U}_{i-1}\mathbf{V}_{i-1}^T$
                \State $l \leftarrow \dim(\mathbf{U}_{i-1}\mathbf{V}_{i-1}^T)$
                \State $\mathbf{g} \leftarrow \mathbf{U}_{i-1}\mathbf{V}_{i-1}^T\mathbf{x}[o_c+1:o_c+l]$
                \State $\mathbf{a}[o_r+1:o_r+l] = \mathbf{a}[o_r+1:o_r+l] - \mathbf{g}$
                \State $\mathbf{b}[o_r+1:o_r+l] = \mathbf{a}[o_r+1:o_r+l] - \mathbf{g}$
            \EndIf
            \State $\mathbf{a}_i \leftarrow \mathbf{a}[j+1:j+m]$
            \State $\mathbf{b}_i \leftarrow \mathbf{b}[j+1:j+m]$
            \State $P = P*\Phi_m(\mathbf{a}_i, \mathbf{b}_i; \mathbf{0}, \mathbf{B}_i\mathbf{B}_i^T)$
            \State $\mathbf{x}[j+1:j+m] \leftarrow \mathbf{B}_{i}^{-1} \expt{\mathbf{X}_i}$
        \EndFor
    \EndProcedure

    \end{algorithmic}\label{alg:hmvn}
\end{algorithm}

Note the probabilities $\Phi_m(\mathbf{a}_i, \mathbf{b}_i; \mathbf{0}, \mathbf{B}_i\mathbf{B}_i^T)$ can be computed using Quasi-Monte Carlo method (\texttt{HMVN}, Method 1 in \citet{cao2019hierarchical}),  $d$-dimensional conditioning algorithm (\texttt{HCMVN}, Method 2 in \citet{cao2019hierarchical}) or with $d$-dimensional conditioning algorithm with univariate reordering (\texttt{HRCMVN}, Method 3 in \citet{cao2019hierarchical}). These methods are more effective and easily parallelizable than the classical methods.

\subsection{Computational Complexity}

For a clearer comparison of the complexities, we decompose the complexity of Algorithm \ref{alg:hmvn} into three parts and list the complexity for each part in Table \ref{tbl:cc_hmvn}, where $M(\cdot)$ denotes the complexity of the QMC simulation in the given dimension. 
%Table \ref{tbl:cc_hmvn} shows that the time efﬁciency of the $d$-dimensional conditioning algorithm mainly comes from lowering the dimension in which the QMC simulation is performed. 
\renewcommand{\arraystretch}{1.5}
\begin{table}[ht]
    \begin{center}
        \begin{tabular}{l l l l}
                            & MVN prob                     & Trunc exp          & Upd limits            \\
            \hline
            \texttt{HMVN}   & $\frac{n}{m} M(m)$           & $2nM(m) + O(nm^2)$ & $O(mn + kn log(n/m))$ \\
            \texttt{HCMVN}  & $\frac{n}{d} M(d) + O(m^2n)$ & $2nM(d) + O(nd^2)$ & $O(mn + kn log(n/m))$ \\
            \texttt{HRCMVN} & $\frac{n}{d} M(d) + O(m^2n)$ & $2nM(d) + O(nd^2)$ & $O(mn + kn log(n/m))$ \\
            \hline
        \end{tabular}
        \caption{Complexity decomposition of the \texttt{HMVN}, \texttt{HCMVN}, and \texttt{HRCMVN}}\label{tbl:cc_hmvn}
    \end{center}
\end{table}
\renewcommand{\arraystretch}{1}


The three parts of the complexity are the calculation of the MVN probability (MVN prob), the calculation of the truncated expectations (Trunc exp), and the update of the integration limits with truncated expectations (Upd limits). The latter two share the same asymptotic order in all three complexity terms. The updating cost is independent of the method. The complexity of the univariate reordering is $O(m^2 n)$, the same as the complexity of computing the MVN probabilities in \texttt{HCMVN}, resulting in an identical major complexity component for \texttt{HCMVN} and \texttt{HRCMVN}. Since \texttt{HCMVN} and \texttt{HRCMVN} perform the QMC  simulation in $d$-dimensions, these two methods are not greatly affected by the choice of $m$.