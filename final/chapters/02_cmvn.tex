\section{Multidimensional Conditioning Approximations}

\subsection{Quasi Monte Carlo Method}\label{subsec:qmc}

Error bound of $O(N^{-1/2})$ for monte carlo(MC) method which is guaranteed by central limit theorem is not enough for high dimensional multivariate random variable. \citet{genz2009computation} claimed independent sample points is the reason of slow convergence. \textit{Quasi-Monte Carlo}(QMC) methods uses deterministic sequences of sample points. Via employing low discrepancy sets for sequence, QMC is asymptotically efficient than MC.
Since square root of prime numbers is irrational and linear independent over the rational numbers, $K_N$ defined below is low-discrepancy sets.
$$K_N=\{i\mathbf{q}\text{ mod }1,i=1,\cdots,N\},$$
where $\mathbf{q}=\sqrt{\mathbf{p}}$ and $\mathbf{p}$ is set of prime numbers. Selecting $K_N$ shifted with $\boldsymbol{\Delta}\sim U[0,1]^n$ as sample points sequence, we have been implemented QMC.
$$L_N=\{\mathbf{z}+\boldsymbol{\Delta}\text{ mod }1:\mathbf{z}\in K_N\}$$

\begin{figure}[ht]
	\centering
	\includegraphics[width=.5\linewidth]{figs/QMC.jpg}
	\caption{Comparison of MC and QMC sample points\citep{genz2009computation}}
	\label{fig:QMC}
\end{figure}

Cholesky decomposition and few tranformation gives (\ref{eqn:qmc}), and algorithm \label{ref:QMC} implemented (\ref{eqn:qmc}) at $L_N \times ns$ times.

\begin{align}
\boldsymbol{\Phi}_n(\mathbf{a}\leq\mathbf{x}\leq\mathbf{b};\boldsymbol{\Sigma})
&=\boldsymbol{\Phi}_n(a\leq\mathbf{Ly}\leq\mathbf{b};I_n)\nonumber\\
&=\int_{a_1\leq l_{11}y_1\leq b_1}\phi(y_1)\cdots\int_{a_n\leq\mathbf{l}_n^t\mathbf{y}\leq b_n}\phi(y_n)d\mathbf{y}\nonumber\\
&=\int_{\tilde{a}_1}^{\tilde{b}_1}\phi(y_1)
\int_{\tilde{a}_2(y_1)}^{\tilde{b}_2(y_1)}\phi(y_2)\cdots
\int_{\tilde{a}_n(y_1,\cdots,y_{n-1})}^{\tilde{b}_n(y_1,\cdots,y_{n-1})}\phi(y_n)d\mathbf{y}\nonumber\\
\text{with } & \tilde{a}_i(y_1,\cdots,y_{i-1})=\frac{a_i-\sum_{j=1}^{i-1}l_{ij}y_j}{l_{ii}}\text{ and }
(\tilde{b}_i(y_1,\cdots,y_{i-1}))=\frac{b_i-\sum_{j=1}^{i-1}l_{ij}y_j}{l_{ii}}\nonumber\\
\intertext{}
&=\int_{\Phi(\tilde{a}_1)}^{\Phi(\tilde{b}_1)}
\int_{\Phi(\tilde{a}_2(\Phi^{-1}(z_1)))}^{\Phi(\tilde{b}_2(\Phi^{-1}(z_1)))}\cdots
\int_{\Phi(\tilde{a}_n(\Phi^{-1}(z_1),\cdots,\Phi^{-1}(z_{n-1})))}^{\Phi(\tilde{b}_n(\Phi^{-1}(z_1),\cdots,\Phi^{-1}(z_{n-1})))}
d\mathbf{z}(y_i=\Phi^{-1}(z_i))\nonumber\nonumber\\
&=(e_1-d_1)\int_0^1(e_2(w_1)-d_2(w_1))\cdots\nonumber\\
&\int_0^1(e_n(w_1,\cdots,w_{n-1}) - d_n(w_1,\cdots,w_{n-1})))\int_0^1d\mathbf{w}\nonumber\\
\text{with } & z_i=d_i+(e_i-d_i)w_i 
\label{eqn:qmc}
\end{align}

\begin{algorithm}[ht]
	\caption{Multivariate Normal Probability with Quasi Monte Carlo Method}
	\begin{algorithmic}[1]
		\Procedure{\texttt{MVN}}{$\boldsymbol{\mu}, \boldsymbol{\Sigma}, \mathbf{a}, \mathbf{b}, ns, N$}
		\State $\mathbf{L}=\text{cholesky}(\boldsymbol{\Sigma})$
		\State $\mathbf{a}=\mathbf{a}-\boldsymbol{\mu}$; $\mathbf{b}=\mathbf{b}-\boldsymbol{\mu}$
		\State $T=0,N=0,V=0$
		\State $\mathbf{p}$ = vector of primes less than $\frac{5n\log{n+1}}{4}$;$\mathbf{q}=\sqrt{\mathbf{p}}$
		\State $\mathbf{P}=\mathbf{1}_{ns}$
		\State $ans = 0$
		\For{$i=1,\cdots,ns$}
		\State $I_i=0$, $\boldsymbol{\Delta}\sim U(0,1)^n$
		\For{$j=1,\cdots,N$}
		\State $\mathbf{X}[1:n,j]=(j+1)\mathbf{q}+\boldsymbol{\Delta}$
		\State $\mathbf{X}[1:n,j]=2\lvert\mathbf{X}[1:n,j]-\text{floor}(\mathbf{X}[1:n,j])\rvert-1$
		\EndFor
		\State $\mathbf{sample}=\mathbf{O}_{n,N}$
		\State $\mathbf{s},\mathbf{c},\mathbf{d},\mathbf{dc}, \mathbf{P}=\mathbf{0}_N$
		\For{$j=1, \cdots,n$}
		\If{$j>1$}
		\State $c=\min(1, c + X[j-1,:]\odot dc)$
		\State $\mathbf{sample}[i-1,1:N] = \Phi^{-1}(c)$
		\State $s = \mathbf{sample}[1:i-1,1:N]^TL[1:i-1, i]$
		\EndIf
		\State $\mathbf{P} *= \Phi(\frac{b-s}{L[i,i]}) - \Phi(\frac{a-s}{L[i,i]})$
		\EndFor
		\State $\text{ans} += \text{mean}(\mathbf{P})$
		\EndFor
		\State\Return $\text{ans}/ns$
		\EndProcedure
	\end{algorithmic}\label{alg:QMC}
\end{algorithm}


\subsection{Conditioning Approximation}

We can exploit Cholesky factors from LDL decomposition rather than dealing with original covariance matrix. \citet{mendell1974multifactorial} and \citet{kamakura1989estimation} developed conditioning method to calculated cdf of multivariate truncated normal distribution. \citet{trinh2015bivariate} employ bivariate blocking method for efficient calculation while accuracy is preserved

$$\boldsymbol{\Sigma} = \begin{pmatrix}
\boldsymbol{\Sigma}_{1,1} & \mathbf{R}^T\\
\mathbf{R} & \hat{\boldsymbol{\Sigma}}
\end{pmatrix}\text{, with } \mathbf{L}=\begin{pmatrix}
\mathbf{I}_{2} & \mathbf{O}\\1:
\mathbf{M} & \mathbf{L}
\end{pmatrix}\text{ and } \mathbf{D}=\begin{pmatrix}
\mathbf{D}_{1} & \mathbf{O}\\
\mathbf{O} & \mathbf{\hat{D}}
\end{pmatrix},$$
where $\boldsymbol{\Sigma}_{1,1}, \mathbf{D}_{1}$ is a $2\times2$ matrix. From $\mathbf{D}_1=\boldsymbol{\Sigma_{1,1}}$, $\mathbf{M}=\mathbf{R}\mathbf{D}_1^{-1}$, $\mathbf{\hat{D}}=\hat{\boldsymbol{\Sigma}}-\mathbf{M}\mathbf{D}_1\mathbf{M}^T$, we can obtain bivariate LDL decomposition of $\boldsymbol{\Sigma}$ inductively.\\
With transformation $\mathbf{y}=L\mathbf{x}$, $\mathbf{a}\leq\mathbf{x}\leq\mathbf{b}$ is tranformed to $a_j-\sum_{m=1}^{j-1}l_{jm}x_m=\alpha_j\leq x_j\leq b_j-\sum_{m=1}^{j-1}l_{jm}x_m=\beta_j$ for $j=1,\cdots,n$. Then, with $k=\frac{n}{2}$ and $\mathbf{x}_{2k}=(x_{2k-1},x_{2k})^T$
\begin{align}\label{eqn:phi_cond-biv}
	\boldsymbol{\Phi}_n(\mathbf{a},\mathbf{b};\mathbf{0},\boldsymbol{\Sigma})
	&= \frac{1}{\sqrt{\lvert\mathbf{D}\rvert(2\pi)^n}}\int_{\alpha_1}^{\beta_1}\int_{\alpha_2}^{\beta_2}e^{-\frac{1}{2}\mathbf{x_2}^T\mathbf{D}_1^{-1}\mathbf{x}_2}\nonumber\\
	&\cdots \int_{\alpha_{2k-1}}^{\beta_{2k-1}}\int_{\alpha_{2k}}^{\beta_{2k}}e^{-\frac{1}{2}\mathbf{x_{2k}}^T\mathbf{D}_1^{-1}\mathbf{x}_{2k}}
\end{align}
\citet{cao2019hierarchical} generalizes bivariate method of \citet{trinh2015bivariate} to $d$-dimensional. Algorithms and details are following.
\begin{algorithm}[ht]
	\caption{LDL decomposition}
	\begin{algorithmic}[1]
		\Procedure{\texttt{LDL}}{$\boldsymbol{\Sigma}$}
		\State $\mathbf{L} \leftarrow \mathbf{I}_m, \mathbf{D} \leftarrow \mathbf{O}_m$
		\For{$i = 1:d:m-d+1$}
		\State $\mathbf{D}[i:i+d-1,i:i+d-1] \leftarrow \boldsymbol{\Sigma}[i:i+d-1,i:i+d-1]$
		\State $\mathbf{L}[i+d:m,i:i+d-1] \leftarrow \boldsymbol{\Sigma}[i+d:m,i:i+d-1]\mathbf{D}^{-1}[i:i+d-1,i:i+d-1]$
		\State $\boldsymbol{\Sigma}[i+d:m,i+d:m]\leftarrow\boldsymbol{\Sigma}[i+d:m,i+d:m]-\mathbf{L}[i+d:m,i:i+d-1] \mathbf{D}^{-1}[i:i+d-1,i:i+d-1] \mathbf{L}[i:i+d-1,i+d:m]$
		\If{$i+d<m$}
			\State $\mathbf{D}[i+d:m,i+d:m] \leftarrow \boldsymbol{\Sigma}[i+d:m,i+d:m]$
		\EndIf
		\EndFor
		\State\Return $\mathbf{L}$ and $\mathbf{D}$
		\EndProcedure
		
	\end{algorithmic}\label{alg:LDL-d}
\end{algorithm}
When $s=\frac{m}{d}$ is integer, results of Algorithm \ref{alg:LDL-d}, $\mathbf{L}, \mathbf{D}$ can be written as
$$
\mathbf{L} = \begin{pmatrix}
\mathbf{I}_d & \mathbf{O}_d & \cdots &\mathbf{O}_d\\
\mathbf{L}_{2,1} & \ddots & \ddots &\vdots\\
\vdots & \ddots & \mathbf{I}_d & \mathbf{O}_d\\
\mathbf{L}_{s,1} & \cdots & \mathbf{L}_{s,s-1} &\mathbf{I}_d\\
\end{pmatrix},
\mathbf{D} = \begin{pmatrix}
\mathbf{D}_1 & \mathbf{O}_d & \cdots &\mathbf{O}_d\\
\mathbf{O}_{d} & \ddots & \ddots &\vdots\\
\vdots & \ddots & \mathbf{D}_{s-1} & \mathbf{O}_d\\
\mathbf{O}_d & \cdots & \mathbf{O}_d &\mathbf{D}_s\\
\end{pmatrix}
$$
with $d$-dimensional identitiy matrix $\mathbf{I}_d$ and $d$-dimensional zero matrix $\mathbf{O}_d$ and $d$-dimensional positive-definite matrix $\mathbf{D}_1,\cdots,\mathbf{D}_s$. Algorithm \ref{alg:LDL-d} is still valid when $m$ is not multiple of $d$ if we allow $\mathbf{L},\mathbf{D}$ to have non-$d$ dimensional matrix block as last row.\\
As in \eqref{eqn:phi_cond-biv}, tranformation, $Y=LX$ provides $m$-dimensional multivariate normal prabability as the product of s $d$-dimensional multivariate normal probabilities as below.
\begin{equation}\label{eqn::phi_cond-ddim}
	\boldsymbol{\Phi_m}(\mathbf{a},\mathbf{b};\mathbf{0},\boldsymbol{\Sigma})=\int_{\mathbf{\alpha}_1}^{\mathbf{\beta}_1}\phi_d(\mathbf{y}_1;\mathbf{D}_1)\int_{\mathbf{\alpha}_2}^{\mathbf{\beta}_2}\phi_d(\mathbf{y}_2;\mathbf{D}_2)\cdots\int_{\mathbf{\alpha}_s}^{\mathbf{\beta}_s}\phi_d(\mathbf{y}_s;\mathbf{D}_s)d\mathbf{y}_s\cdots d\mathbf{y}_2d\mathbf{y}_1,
\end{equation}
where $\boldsymbol{\alpha}_i=\mathbf{a}_i-\sum_{j=1}^{i-1}\mathbf{L}_{ij}\mathbf{y}_j, \boldsymbol{\beta}_i=\mathbf{b}_i-\sum_{j=1}^{i-1}\mathbf{L}_{ij}\mathbf{y}_j$
Equation \eqref{eqn::phi_cond-ddim} is implemented as below.

\begin{algorithm}[ht]
	\caption{d-dimensional conditioning algorithm}
	\begin{algorithmic}[1]
		\Procedure{\texttt{CMVN}}{$\boldsymbol{\Sigma},\mathbf{a},\mathbf{b},d$}
		\State $\mathbf{y}\leftarrow\mathbf{0},P\leftarrow1$
		\For{$i = 1:s$}
		\State $j\leftarrow(i-1)d$
		\State $\mathbf{g}\leftarrow\mathbf{L}[j+1:j+d,1:j]\mathbf{y}[1:j]$
		\State $\boldsymbol{\alpha}\leftarrow\mathbf{a}[j+1:j+d]-\mathbf{g}$
		\State $\boldsymbol{\beta}\leftarrow\mathbf{b}[j+1:j+d]-\mathbf{g}$
		\State $\mathbf{D}^\prime\leftarrow\mathbf{D}[j+1:j+d,j+1:j+d]$
		\State $P\leftarrow P\cdot\boldsymbol{\Phi}_d(\boldsymbol{\alpha},\boldsymbol{\beta};\mathbf{0},\mathbf{D}^\prime)$
		\State $\mathbf{y}[j+1:j+d]\leftarrow E[\mathbf{Y}^\prime]$
		\EndFor
		\State\Return $P$ and $\mathbf{y}$
		\EndProcedure
	\end{algorithmic}\label{alg:CMVN}
\end{algorithm}

\subsection{Multidimensional Truncated Expectations}
In algorithm \ref{alg:CMVN}, approximation of $\boldsymbol{\Phi}_d$ and $E[Y^\prime] is required$. $\boldsymbol{\Phi}_d$ is possibly obtained with quasi monte calro method proposed by \citet{genz1992numerical}, and \citet{kan2017moments} provides methods to calculate $E[Y^\prime]$. The truncated expectation can be expressed by
$$E(X^{e_j})=\frac{1}{\boldsymbol{\Phi}(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\boldsymbol{\Sigma})}\int_\mathbf{a}^\mathbf{b}x_j\phi_d(\mathbf{x};\boldsymbol{\mu},\boldsymbol{\Sigma})d\mathbf{x}=\frac{1}{\boldsymbol{\Phi}(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\boldsymbol{\Sigma})}F_j^d(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\boldsymbol{\Sigma})$$

\begin{theorem}\label{thm:thmkan}\citep{kan2017moments}
$$F_j^d(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\boldsymbol{\Sigma})= \mu_j\boldsymbol{\Phi}_d(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\boldsymbol{\Sigma})+\mathbf{e}_j^T\boldsymbol{\Sigma}\mathbf{c}$$
,where $c$ is a vector with lth component defined as
$$\begin{aligned}
c_l&=\phi_1(a_l;\mu_l,\sigma_l^2)\Phi_{d-1}(\mathbf{a}_{-l},\mathbf{b}_{-l};\boldsymbol{\hat{\mu}}^1, \hat{\boldsymbol{\Sigma}}_l)\\
&-\phi_1(b_l;\mu_l,\sigma_l^2)\Phi_{d-1}(\mathbf{a}_{-l},\mathbf{b}_{-l};\boldsymbol{\hat{\mu}}^2, \hat{\boldsymbol{\Sigma}}_l)\\
\boldsymbol{\hat{\mu}}^1_l&=\mu_{-l}+\boldsymbol{\Sigma}_{-l,l}\frac{a_l-\mu_l}{\sigma_l^2},\\
\boldsymbol{\hat{\mu}}^2_l&=\mu_{-l}+\boldsymbol{\Sigma}_{-l,l}\frac{b_l-\mu_l}{\sigma_l^2},\\
\hat{\boldsymbol{\Sigma}}_l&=\boldsymbol{\Sigma}_{-l,-l} -\frac{1}{\sigma_l^2}\boldsymbol{\Sigma}_{-l,l}\boldsymbol{\Sigma}_{l,-l}
\end{aligned}$$
\end{theorem}
\begin{proof}
	Derivative of the multivariate normal density satisfies below
	\begin{equation}\label{eqn:proof}
	-\frac{\partial\phi_n(\mathbf{x};\boldsymbol{\mu},\boldsymbol{\Sigma})}{\partial\mathbf{x}}=\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\phi_n(\mathbf{x};\boldsymbol{\mu},\boldsymbol{\Sigma})
	\end{equation}
	With integration \eqref{eqn:proof} from $\mathbf{a}$ to $\mathbf{b}$,
	\begin{equation}\label{eqn:proof2}
	\boldsymbol{c}=\boldsymbol{\Sigma}^{-1}\begin{bmatrix}
	F_1^d-\mu_1\Phi_{d-1}\\
	F_2^d-\mu_1\Phi_{d-1}\\
	\vdots\\
	F_d^d-\mu_1\Phi_{d-1}
	\end{bmatrix}
	\end{equation}
	Using the fact that
	$$\begin{aligned}
	\phi_n(\mathbf{x};\boldsymbol{\mu},\boldsymbol{\Sigma})\vert_{x_j=a_j}&=\phi_1(a_j;\mu_j,\sigma_j^2)\phi_{n-1}(\mathbf{x}_{-j};\boldsymbol{\hat{\mu}}^1_j\hat{\boldsymbol{\Sigma}}^1)\\
	\phi_n(\mathbf{x};\boldsymbol{\mu},\boldsymbol{\Sigma})\vert_{x_j=b_j}&=\phi_1(b_j;\mu_j,\sigma_j^2)\phi_{n-1}(\mathbf{x}_{-j};\boldsymbol{\hat{\mu}}^2_j\hat{\boldsymbol{\Sigma}}^1),
	\end{aligned}$$
	\eqref{eqn:proof2} becomes
	$$\begin{aligned}
	c_l&=\phi_1(a_l;\mu_l,\sigma_l^2)\Phi_{d-1}(\mathbf{a}_{-l},\mathbf{b}_{-l};\boldsymbol{\hat{\mu}}^1, \hat{\boldsymbol{\Sigma}}_l)\\
	&-\phi_1(b_l;\mu_l,\sigma_l^2)\Phi_{d-1}(\mathbf{a}_{-l},\mathbf{b}_{-l};\boldsymbol{\hat{\mu}}^2, \hat{\boldsymbol{\Sigma}}_l)
	\end{aligned}$$
\end{proof}
Theorem \ref{thm:thmkan} has same form with bivariate version of \citet{trinh2015bivariate} with $d=2$ and it allows us to calculate $E[Y^\prime]$ in Algorithm \ref{alg:CMVN} with $\boldsymbol{\Phi}$ which can be obtained with quasi monte calro method proposed by \citet{genz1992numerical}

\subsection{Multidimensional Conditioning Approximation with Univariate Reordering}
It is known that appropriate integration order on conditioning algorithm possibly improves estimation accuracy, by reducing overall variation. \citet{schervish1984algorithm} originally proposed to arrange variables having relatively short integration interval width than other variables should be integrated later. Also, \citet{gibson1994monte} suggested variables which have smallest expected values should be placed in outermost postion of the whole integration. Since, inner integrals which have smaller variation have the most influence with this order, overall variance decreases.
\citet{trinh2015bivariate} also employs this ordering, and \citet{cao2019hierarchical} generalized it to $d$-dimensional problem.
\begin{algorithm}[ht]
	\caption{d-dimensional conditioning algorithm with univariate reordering}
	\begin{algorithmic}[1]
		\Procedure{\texttt{RCMVN}}{$\boldsymbol{\Sigma},\mathbf{a},\mathbf{b},d$}
		\State $\mathbf{y}\leftarrow\mathbf{0},\mathbf{C}\leftarrow\boldsymbol{\Sigma}$
		\For{$i = 1:m$}
		\If{$i > 1$}
		\State $\mathbf{y}[i-1]\leftarrow\frac{\phi(a^\prime)-\phi(b^\prime)}{\Phi(b^\prime)-\Phi(a^\prime)}$
		\EndIf
		\State $j\leftarrow\text{argmin}_{i\leq j\leq m}\{\Phi(\frac{\mathbf{b}[j]-\mathbf{C}[j,1:i-1]\mathbf{y}[1:i-1]}{\sqrt{\boldsymbol{\Sigma}[j,j]-\mathbf{C}[j,1:i-1]\mathbf{C}^T[j,1:i-1]}})-\Phi(\frac{\mathbf{a}[j]-\mathbf{C}[j,1:i-1]\mathbf{y}[1:i-1]}{\sqrt{\boldsymbol{\Sigma}[j,j]-\mathbf{C}[j,1:i-1]\mathbf{C}^T[j,1:i-1]}})\}$
		\State $\boldsymbol{\Sigma}[:,(i,j)]\leftarrow\boldsymbol{\Sigma}[:,(j,i)]$;$\boldsymbol{\Sigma}[(i,j),:]\leftarrow\boldsymbol{\Sigma}[(j,i),:]$
		\State $\mathbf{C}[:,(i,j)]\leftarrow\mathbf{C}[:,(j,i)]$;$\mathbf{C}[(i,j),:]\leftarrow\mathbf{C}[(j,i),:]$
		\State $\mathbf{a}[(i,j)]=\mathbf{a}[(j,i)]$
		\State $\mathbf{b}[(i,j)]=\mathbf{b}[(j,i)]$
		\State $\mathbf{C}[i,i]\leftarrow\sqrt{\boldsymbol{\Sigma}[i,i]-\mathbf{C}[i,1:i-1]\mathbf{C}^T[i,1:i-1]}$
		\State $\mathbf{C}[j,i]\leftarrow \frac{\boldsymbol{\Sigma}[j,i]-\mathbf{C}[i,1:i-1]\mathbf{C}^T[j,1:i-1]}{\mathbf{C}[i,i]}$, for $j=i+1,\cdots,m$
		\State $a^\prime=\frac{\mathbf{a}[i]-\mathbf{C}[i,1:i-1]y[1:i-1]}{\mathbf{C[i,i]}}$
		\State $b^\prime=\frac{\mathbf{b}[i]-\mathbf{C}[i,1:i-1]y[1:i-1]}{\mathbf{C[i,i]}}$
		\EndFor
		\State\Return \texttt{CMVN}($\boldsymbol{\Sigma},\mathbf{a},\mathbf{b},d$) as in Algorithm \ref{alg:CMVN}
		\EndProcedure
	\end{algorithmic}\label{alg:RCMVN}
\end{algorithm}
