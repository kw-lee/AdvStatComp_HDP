\section{Multidimensional Conditioning Approximations}

\subsection{Conditioning Approximation}

We can exploit Cholesky factors from LDL decomposition rather than dealing with original covariance matrix. \citet{mendell1974multifactorial} and \citet{kamakura1989estimation} developed conditioning method to calculated cdf of multivariate truncated normal distribution. \citet{trinh2015bivariate} employ bivariate blocking method for efficient calculation while accuracy is preserved.\\
$$\mathbf{\Sigma} = \begin{pmatrix}
\mathbf{\Sigma}_{1,1} & \mathbf{R}^T\\
\mathbf{R} & \mathbf{\hat{\Sigma}}
\end{pmatrix}\text{, with } \mathbf{L}=\begin{pmatrix}
\mathbf{I}_{2} & \mathbf{O}\\1:
\mathbf{M} & \mathbf{L}
\end{pmatrix}\text{ and } \mathbf{D}=\begin{pmatrix}
\mathbf{D}_{1} & \mathbf{O}\\
\mathbf{O} & \mathbf{\hat{D}}
\end{pmatrix}$$
,where $\mathbf{\Sigma}_{1,1}, \mathbf{D}_{1}$ is a $2\times2$ matrix. From $\mathbf{D}_1=\mathbf{\Sigma_{1,1}}$, $\mathbf{M}=\mathbf{R}\mathbf{D}_1^{-1}$, $\mathbf{\hat{D}}=\mathbf{\hat{\Sigma}}-\mathbf{M}\mathbf{D}_1\mathbf{M}^T$, we can obtain bivariate LDL decomposition of $\mathbf{\Sigma}$ inductively.\\
With transformation $\mathbf{y}=L\mathbf{x}$, $\mathbf{a}\leq\mathbf{x}\leq\mathbf{b}$ is tranformed to $a_j-\sum_{m=1}^{j-1}l_{jm}x_m=\alpha_j\leq x_j\leq b_j-\sum_{m=1}^{j-1}l_{jm}x_m=\beta_j$ for $j=1,\cdots,n$. Then, with $k=\frac{n}{2}$ and $\mathbf{x}_{2k}=(x_{2k-1},x_{2k})^T$
\begin{align}\label{eqn:phi_cond-biv}
	\boldsymbol{\Phi}_n(\mathbf{a},\mathbf{b};\mathbf{0},\mathbf{\Sigma})
	&= \frac{1}{\sqrt{\lvert\mathbf{D}\rvert(2\pi)^n}}\int_{\alpha_1}^{\beta_1}\int_{\alpha_2}^{\beta_2}e^{-\frac{1}{2}\mathbf{x_2}^T\mathbf{D}_1^{-1}\mathbf{x}_2}\nonumber\\
	&\cdots \int_{\alpha_{2k-1}}^{\beta_{2k-1}}\int_{\alpha_{2k}}^{\beta_{2k}}e^{-\frac{1}{2}\mathbf{x_{2k}}^T\mathbf{D}_1^{-1}\mathbf{x}_{2k}}
\end{align}
\citet{cao2019hierarchical} generalizes bivariate method of \citet{trinh2015bivariate} to $d$-dimensional. Algorithms and details are following.
\begin{algorithm}[H]
	\caption{LDL decomposition}
	\begin{algorithmic}[1]
		\Procedure{\texttt{LDL}}{$\Sigma$}
		\State $\mathbf{L} \leftarrow \mathbf{I}_m, \mathbf{D} \leftarrow \mathbf{O}_m$
		\For{$i = 1:d:m-d+1$}
		\State $\mathbf{D}[i:i+d-1,i:i+d-1] \leftarrow \mathbf{\Sigma}[i:i+d-1,i:i+d-1]$
		\State $\mathbf{L}[i+d:m,i:i+d-1] \leftarrow \mathbf{\Sigma}[i+d:m,i:i+d-1]\mathbf{D}^{-1}[i:i+d-1,i:i+d-1]$
		\State $\mathbf{\Sigma}[i+d:m,i+d:m]\leftarrow\mathbf{\Sigma}[i+d:m,i+d:m]-\mathbf{L}[i+d:m,i:i+d-1] \mathbf{D}^{-1}[i:i+d-1,i:i+d-1] \mathbf{L}[i:i+d-1,i+d:m]$
		\If{$i+d<m$}
			\State $\mathbf{D}[i+d:m,i+d:m] \leftarrow \mathbf{\Sigma}[i+d:m,i+d:m]$
		\EndIf
		\EndFor
		\State\Return $\mathbf{L}$ and $\mathbf{D}$
		\EndProcedure
		
	\end{algorithmic}\label{alg:LDL-d}
\end{algorithm}
When $s=\frac{m}{d}$ is integer, results of Algorithm \ref{alg:LDL-d}, $\mathbf{L}, \mathbf{D}$ can be written as
$$
\mathbf{L} = \begin{pmatrix}
\mathbf{I}_d & \mathbf{O}_d & \cdots &\mathbf{O}_d\\
\mathbf{L}_{2,1} & \ddots & \ddots &\vdots\\
\vdots & \ddots & \mathbf{I}_d & \mathbf{O}_d\\
\mathbf{L}_{s,1} & \cdots & \mathbf{L}_{s,s-1} &\mathbf{I}_d\\
\end{pmatrix},
\mathbf{D} = \begin{pmatrix}
\mathbf{D}_1 & \mathbf{O}_d & \cdots &\mathbf{O}_d\\
\mathbf{O}_{d} & \ddots & \ddots &\vdots\\
\vdots & \ddots & \mathbf{D}_{s-1} & \mathbf{O}_d\\
\mathbf{O}_d & \cdots & \mathbf{O}_d &\mathbf{D}_s\\
\end{pmatrix}
$$
with $d$-dimensional identitiy matrix $\mathbf{I}_d$ and $d$-dimensional zero matrix $\mathbf{O}_d$ and $d$-dimensional positive-definite matrix $\mathbf{D}_1,\cdots,\mathbf{D}_s$. Algorithm \ref{alg:LDL-d} is still valid when $m$ is not multiple of $d$ if we allow $\mathbf{L},\mathbf{D}$ to have non-$d$ dimensional matrix block as last row.\\
As in \eqref{eqn:phi_cond-biv}, tranformation, $Y=LX$ provides $m$-dimensional multivariate normal prabability as the product of s $d$-dimensional multivariate normal probabilities as below.
\begin{equation}\label{eqn::phi_cond-ddim}
	\boldsymbol{\Phi_m}(\mathbf{a},\mathbf{b};\mathbf{0},\mathbf{\Sigma})=\int_{\mathbf{\alpha}_1}^{\mathbf{\beta}_1}\phi_d(\mathbf{y}_1;\mathbf{D}_1)\int_{\mathbf{\alpha}_2}^{\mathbf{\beta}_2}\phi_d(\mathbf{y}_2;\mathbf{D}_2)\cdots\int_{\mathbf{\alpha}_s}^{\mathbf{\beta}_s}\phi_d(\mathbf{y}_s;\mathbf{D}_s)d\mathbf{y}_s\cdots d\mathbf{y}_2d\mathbf{y}_1
\end{equation}
,where $\boldsymbol{\alpha}_i=\mathbf{a}_i-\sum_{j=1}^{i-1}\mathbf{L}_{ij}\mathbf{y}_j, \boldsymbol{\beta}_i=\mathbf{b}_i-\sum_{j=1}^{i-1}\mathbf{L}_{ij}\mathbf{y}_j$
Equation \eqref{eqn::phi_cond-ddim} is implemented as below.

\begin{algorithm}[H]
	\caption{d-dimensional conditioning algorithm}
	\begin{algorithmic}[1]
		\Procedure{\texttt{CMVN}}{$\mathbf{\Sigma},\mathbf{a},\mathbf{b},d$}
		\State $\mathbf{y}\leftarrow\mathbf{0},P\leftarrow1$
		\For{$i = 1:s$}
		\State $j\leftarrow(i-1)d$
		\State $\mathbf{g}\leftarrow\mathbf{L}[j+1:j+d,1:j]\mathbf{y}[1:j]$
		\State $\boldsymbol{\alpha}\leftarrow\mathbf{a}[j+1:j+d]-\mathbf{g}$
		\State $\boldsymbol{\beta}\leftarrow\mathbf{b}[j+1:j+d]-\mathbf{g}$
		\State $\mathbf{D}^\prime\leftarrow\mathbf{D}[j+1:j+d,j+1:j+d]$
		\State $P\leftarrow P\cdot\boldsymbol{\Phi}_d(\boldsymbol{\alpha},\boldsymbol{\beta};\mathbf{0},\mathbf{D}^\prime)$
		\State $\mathbf{y}[j+1:j+d]\leftarrow E[\mathbf{Y}^\prime]$
		\EndFor
		\State\Return $P$ and $\mathbf{y}$
		\EndProcedure
	\end{algorithmic}\label{alg:CMVN}
\end{algorithm}

\subsection{Multidimensional Truncated Expectations}
In algorithm \ref{alg:CMVN} needs approximation of $\boldsymbol{\Phi}_d$ and $E[Y^\prime]$. $\boldsymbol{\Phi}_d$ is possibly obtained with quasi monte calro method proposed by \citet{genz2009computation}, and \citet{kan2017moments} provides methods to calculate $E[Y^\prime]$. The truncated expectation is expressed as
$$E(X^{e_j})=\frac{1}{\boldsymbol{\Phi}(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\mathbf{\Sigma})}\int_\mathbf{a}^\mathbf{b}x_j\phi_d(\mathbf{x};\boldsymbol{\mu},\mathbf{\Sigma})d\mathbf{x}=\frac{1}{\boldsymbol{\Phi}(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\mathbf{\Sigma})}F_j^d(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\mathbf{\Sigma})$$

\begin{theorem}\label{thm:thmkan}\citep{kan2017moments}
$$F_j^d(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\mathbf{\Sigma})= \mu_j\boldsymbol{\Phi}_d(\mathbf{a},\mathbf{b};\boldsymbol{\mu},\mathbf{\Sigma})+\mathbf{e}_j^T\mathbf{\Sigma}\mathbf{c}$$
,where $c$ is a vector with lth component defined as
$$\begin{aligned}
c_l&=\phi_1(a_l;\mu_l,\sigma_l^2)\Phi_{d-1}(\mathbf{a}_{-l},\mathbf{b}_{-l};\boldsymbol{\hat{\mu}}^1, \mathbf{\hat{\Sigma}}_l)\\
&-\phi_1(b_l;\mu_l,\sigma_l^2)\Phi_{d-1}(\mathbf{a}_{-l},\mathbf{b}_{-l};\boldsymbol{\hat{\mu}}^2, \mathbf{\hat{\Sigma}}_l)\\
\boldsymbol{\hat{\mu}}^1_l&=\mu_{-l}+\mathbf{\Sigma}_{-l,l}\frac{a_l-\mu_l}{\sigma_l^2},\\
\boldsymbol{\hat{\mu}}^2_l&=\mu_{-l}+\mathbf{\Sigma}_{-l,l}\frac{b_l-\mu_l}{\sigma_l^2},\\
\hat{\mathbf{\Sigma}}_l&=\mathbf{\Sigma}_{-l,-l} -\frac{1}{\sigma_l^2}\mathbf{\Sigma}_{-l,l}\mathbf{\Sigma}_{l,-l}
\end{aligned}$$
\end{theorem}
\begin{proof}
	Derivative of the multivariate normal density satisfies below
	\begin{equation}\label{eqn:proof}
	-\frac{\partial\phi_n(\mathbf{x};\boldsymbol{\mu},\mathbf{\Sigma})}{\partial\mathbf{x}}=\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\phi_n(\mathbf{x};\boldsymbol{\mu},\mathbf{\Sigma})
	\end{equation}
	With integration \eqref{eqn:proof} from $\mathbf{a}$ to $\mathbf{b}$,
	\begin{equation}\label{eqn:proof2}
	\boldsymbol{c}=\mathbf{\Sigma}^{-1}\begin{bmatrix}
	F_1^d-\mu_1\Phi_{d-1}\\
	F_2^d-\mu_1\Phi_{d-1}\\
	\vdots\\
	F_d^d-\mu_1\Phi_{d-1}
	\end{bmatrix}
	\end{equation}
	Using the fact that
	$$\begin{aligned}
	\phi_n(\mathbf{x};\boldsymbol{\mu},\mathbf{\Sigma})\vert_{x_j=a_j}&=\phi_1(a_j;\mu_j,\sigma_j^2)\phi_{n-1}(\mathbf{x}_{-j};\boldsymbol{\hat{\mu}}^1_j\mathbf{\hat{\Sigma}}^1)\\
	\phi_n(\mathbf{x};\boldsymbol{\mu},\mathbf{\Sigma})\vert_{x_j=b_j}&=\phi_1(b_j;\mu_j,\sigma_j^2)\phi_{n-1}(\mathbf{x}_{-j};\boldsymbol{\hat{\mu}}^2_j\mathbf{\hat{\Sigma}}^1),
	\end{aligned}$$
	\eqref{eqn:proof2} becomes
	$$\begin{aligned}
	c_l&=\phi_1(a_l;\mu_l,\sigma_l^2)\Phi_{d-1}(\mathbf{a}_{-l},\mathbf{b}_{-l};\boldsymbol{\hat{\mu}}^1, \mathbf{\hat{\Sigma}}_l)\\
	&-\phi_1(b_l;\mu_l,\sigma_l^2)\Phi_{d-1}(\mathbf{a}_{-l},\mathbf{b}_{-l};\boldsymbol{\hat{\mu}}^2, \mathbf{\hat{\Sigma}}_l)
	\end{aligned}$$
\end{proof}
Theorem \ref{thm:thmkan} has same form with bivariate version of \citet{trinh2015bivariate} with $d=2$ and it allows us to calculate $E[Y^\prime]$ in Algorithm \ref{alg:CMVN} with $\boldsymbol{\Phi}$ which can be obtained with quasi monte calro method proposed by \citet{genz2009computation}

\subsection{Multidimensional Conditioning Approximation with Univariate Reordering}
It is known that appropriate integration order on conditioning algorithm possibly improves estiation accuracy. \citet{schervish1984algorithm} originally proposed integral with shortest integration interval widths be the outermost integration variables to reduce overall variation of integrand and \citet{gibson1994monte} suggested variables which have smallest expected values be the outermost integration variables. Since innermost integrals which have smaller variation have the most influence with this order, overall variance reduces.
\citet{trinh2015bivariate} also employs this ordering, and \citet{cao2019hierarchical} generalized it to $d$-dimensional problem.
\begin{algorithm}[H]
	\caption{d-dimensional conditioning algorithm with univariate reordering}
	\begin{algorithmic}[1]
		\Procedure{\texttt{RCMVN}}{$\mathbf{\Sigma},\mathbf{a},\mathbf{b},d$}
		\State $\mathbf{y}\leftarrow\mathbf{0},\mathbf{C}\leftarrow\mathbf{\Sigma}$
		\For{$i = 1:m$}
		\If{$i > 1$}
		\State $\mathbf{y}[i-1]\leftarrow\frac{\phi(a^\prime)-\phi(b^\prime)}{\Phi(b^\prime)-\Phi(a^\prime)}$
		\EndIf
		\State $j\leftarrow\text{argmin}_{i\leq j\leq m}\{\Phi(\frac{\mathbf{b}[j]-\mathbf{C}[j,1:i-1]\mathbf{y}[1:i-1]}{\sqrt{\mathbf{\Sigma}[j,j]-\mathbf{C}[j,1:i-1]\mathbf{C}^T[j,1:i-1]}})-\Phi(\frac{\mathbf{a}[j]-\mathbf{C}[j,1:i-1]\mathbf{y}[1:i-1]}{\sqrt{\mathbf{\Sigma}[j,j]-\mathbf{C}[j,1:i-1]\mathbf{C}^T[j,1:i-1]}})\}$
		\State $\mathbf{\Sigma}[:,(i,j)]\leftarrow\mathbf{\Sigma}[:,(j,i)]$;$\mathbf{\Sigma}[(i,j),:]\leftarrow\mathbf{\Sigma}[(j,i),:]$
		\State $\mathbf{C}[:,(i,j)]\leftarrow\mathbf{C}[:,(j,i)]$;$\mathbf{C}[(i,j),:]\leftarrow\mathbf{C}[(j,i),:]$
		\State $\mathbf{a}[(i,j)]=\mathbf{a}[(j,i)]$
		\State $\mathbf{b}[(i,j)]=\mathbf{b}[(j,i)]$
		\State $\mathbf{C}[i,i]\leftarrow\sqrt{\mathbf{\Sigma}[i,i]-\mathbf{C}[i,1:i-1]\mathbf{C}^T[i,1:i-1]}$
		\State $\mathbf{C}[j,i]\leftarrow \frac{\mathbf{\Sigma}[j,i]-\mathbf{C}[i,1:i-1]\mathbf{C}^T[j,1:i-1]}{\mathbf{C}[i,i]}$, for $j=i+1,\cdots,m$
		\State $a^\prime=\frac{\mathbf{a}[i]-\mathbf{C}[i,1:i-1]y[1:i-1]}{\mathbf{C[i,i]}}$
		\State $b^\prime=\frac{\mathbf{b}[i]-\mathbf{C}[i,1:i-1]y[1:i-1]}{\mathbf{C[i,i]}}$
		\EndFor
		\State\Return CMVN($\mathbf{\Sigma},\mathbf{a},\mathbf{b},d$) as in Algorithm \ref{alg:CMVN}
		\EndProcedure
	\end{algorithmic}\label{alg:RCMVN}
\end{algorithm}
