\section{Hierarchical-Block Approximation}

\begin{frame}{Hierarchical Cholesky Decomposition}
\footnotesize
\citet{hackbusch2015hierarchical} proposed hiarchical matrix and its cholesky decomposition method.
$A=LU$ have the structure
$$\begin{pmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{pmatrix}=\begin{pmatrix}L_{11}&O\\L_{21}&L_{22}\end{pmatrix}\begin{pmatrix}L_{11}^T&L_{12}^T\\O&L_{22}^T\end{pmatrix}$$
with lower triangular matrix $L_{11},L_{22}$.
It leads to four tasks:
\begin{itemize}
	\item[(a)] compute $L_{11}$ via Cholesky decomposition of $A_{11}$
	\item[(b)] compute $L_{12}$ from $L_{21}L_{11}^T = A_{21}$
	\item[(c)] low rank approximation of $L_{12}=UV^T$
	\item[(d)] compute $L_{22}$ via Cholesky decomposition of $A_{22}-L_{21}L_{21}^T$
\end{itemize}

We have applied low rank approximation with svd to (c) each block of its decomposition to make implementation efficiently and save storage while accuracy is preserved.
: i.e. $A=UDV^T=\sum_{i=1}^n d_i u_iv_i^T\approx\sum_{i=1}^k d_i u_iv_i^T$.
\end{frame}

\begin{frame}{Hierarchical Cholesky Decomposition}
Hierachical cholesky decomposition of $n\times n$ matrix into $m\times m$ blocks is implemented like below.
\begin{algorithm}[H]
	\caption{Hierachical cholesky decomposition}
	\begin{algorithmic}[1]
		\tiny
		\Procedure{\texttt{hchol}}{$A$, n,m,rank}
		\For{$i=1:log_2(\frac{n}{m})$}
		\State $nb = n/2^i$
		\State x = 0, y = nb
		\For{$j=1:2^{i-1}$}
		\State $\mathbf{U,D,V} = lowrankSVD(A[xbegin+1:xbegin+nb,ybegin+1:ybegin+nb], rank)$
		\State $\mathbf{A}[x + 1:x + nb, y + 1:y + rank] = \mathbf{UD}$
		\State $\mathbf{A}[x + 1:x + nb, y + rank+1:y + nb] = \mathbf{O}$
		\State $\mathbf{A}[y + 1:y + nb, x + 1:x + rank] = \mathbf{VD}$
		\State $\mathbf{A}[y + 1:y + nb, x + rank+1:x + nb] = \mathbf{O}$
		\State $x += 2nb, y += 2nb$
		\EndFor
		\EndFor
		\EndProcedure
		
	\end{algorithmic}\label{alg:hchol}
\end{algorithm}
\end{frame}

\begin{frame}{The Hierarchical-Block Conditioning Method}
\footnotesize
Let $\phi_m(\mathbf{x}; \boldsymbol{\Sigma})$ be a pdf of the $m$-dimensional normal distribution $N(\mathbf{0}, \boldsymbol{\Sigma})$ and $(\mathbf{B}, \mathbf{U}\mathbf{V}^T)$ be the hierarchical Cholesky decompostion of the covariance matrix $\boldsymbol{\Sigma}$. Then,
\begin{equation}\label{eqn:hmvn}
\Phi_n(\mathbf{a}, \mathbf{b}; \mathbf{0}, \boldsymbol{\Sigma}) 
= \int_{\mathbf{a}_1'}^{\mathbf{b}_1'} \phi_m(\mathbf{x}_1; \mathbf{B}_1\mathbf{B}_1^T) 
\cdots 
\int_{\mathbf{a}_r'}^{\mathbf{b}_r'} \phi_r(\mathbf{x}_r; \mathbf{B}_r\mathbf{B}_r^T) d\mathbf{x}_r \cdots d\mathbf{x}_1.
\end{equation}
,where $\mathbf{a}',~\mathbf{b}'$, $i=1,\cdots,r$, are the corresponding segments of the updated $\mathbf{a}$ and $\mathbf{b}$. 

Note the probabilities $\Phi_m(\mathbf{a}_i, \mathbf{b}_i; \mathbf{0}, \mathbf{B}_i\mathbf{B}_i^T)$ can be computed using
\begin{itemize}
	\item[1.] Quasi-Monte Carlo method (\texttt{HMVN}, Method 1 in \citet{cao2019hierarchical})
	\item[2.] $d$-dimensional conditioning algorithm (\texttt{HCMVN}, Method 2 in \citet{cao2019hierarchical})
	\item[3.] $d$-dimensional conditioning algorithm with univariate reordering (\texttt{HRCMVN}, Method 3 in \citet{cao2019hierarchical}). 
\end{itemize} 
These methods are more effective and easily parallelizable than the classical methods.
\end{frame}

\begin{frame}{The Hierarchical-Block Conditioning Method}
\begin{algorithm}[H]
	\caption{Hierarchical-block conditioning algorithm}
	\begin{algorithmic}[1]
		\tiny
		\Procedure{\texttt{HMVN}}{$a,~b,~\boldsymbol{\Sigma},~d$}
		\State $\mathbf{x} \leftarrow \mathbf{0}$ and $P \leftarrow 1$
		\State $[\mathbf{B}, \mathbf{UV}] \leftarrow$ \texttt{choldecomp\_hmatrix}$(\boldsymbol{\Sigma})$
		\For{$i = 1:r$}
		\State $j \leftarrow (i-1)m$
		\If{$i>1$}
		\State $o_r \leftarrow$ row offset of $\mathbf{U}_{i-1}\mathbf{V}_{i-1}^T$
		\State $o_c \leftarrow$ column offset of $\mathbf{U}_{i-1}\mathbf{V}_{i-1}^T$
		\State $l \leftarrow \dim(\mathbf{U}_{i-1}\mathbf{V}_{i-1}^T)$
		\State $\mathbf{g} \leftarrow \mathbf{U}_{i-1}\mathbf{V}_{i-1}^T\mathbf{x}[o_c+1:o_c+l]$
		\State $\mathbf{a}[o_r+1:o_r+l] = \mathbf{a}[o_r+1:o_r+l] - \mathbf{g}$
		\State $\mathbf{b}[o_r+1:o_r+l] = \mathbf{a}[o_r+1:o_r+l] - \mathbf{g}$
		\EndIf
		\State $\mathbf{a}_i \leftarrow \mathbf{a}[j+1:j+m]$
		\State $\mathbf{b}_i \leftarrow \mathbf{b}[j+1:j+m]$
		\State $P = P*\Phi_m(\mathbf{a}_i, \mathbf{b}_i; \mathbf{0}, \mathbf{B}_i\mathbf{B}_i^T)$
		\State $\mathbf{x}[j+1:j+m] \leftarrow \mathbf{B}_{i}^{-1} E(\mathbf{X}_i)$
		\EndFor
		\EndProcedure
	\end{algorithmic}\label{alg:hmvn}
\end{algorithm}
\end{frame}

\begin{frame}{Computational Complexity}
\footnotesize

$M(\cdot)$ denotes the complexity of the QMC simulation in the given dimension. \\
Table \ref{tbl:cc_hmvn} shows that the time efÔ¨Åciency of the $d$-dimensional conditioning algorithm mainly comes from lowering the dimension in which the QMC simulation is performed. 
\renewcommand{\arraystretch}{1.5}
\begin{table}[H]
	\begin{center}
		\begin{tabular}{l l l l}
			& MVN prob                     & Trunc exp          & Upd limits            \\
			\hline
			\texttt{HMVN}   & $\frac{n}{m} M(m)$           & $2nM(m) + O(nm^2)$ & $O(mn + kn log(n/m))$ \\
			\texttt{HCMVN}  & $\frac{n}{d} M(d) + O(m^2n)$ & $2nM(d) + O(nd^2)$ & $O(mn + kn log(n/m))$ \\
			\texttt{HRCMVN} & $\frac{n}{d} M(d) + O(m^2n)$ & $2nM(d) + O(nd^2)$ & $O(mn + kn log(n/m))$ \\
			\hline
		\end{tabular}
		\caption{Complexity decomposition of the \texttt{HMVN}, \texttt{HCMVN}, and \texttt{HRCMVN}}\label{tbl:cc_hmvn}
	\end{center}
\end{table}
\renewcommand{\arraystretch}{1}

\begin{itemize}
	\item The updating cost is independent of the method.
	\item The complexity of the univariate reordering is $O(m^2 n)$, the same as the complexity of computing the MVN probabilities in \texttt{HCMVN}
	\item Since \texttt{HCMVN} and \texttt{HRCMVN} perform the QMC  simulation in $d$-dimensions, these two methods are not greatly affected by the choice of $m$.
\end{itemize}
\end{frame}
